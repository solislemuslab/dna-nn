{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explanation of data (file it came from, which response is being predicted, which sequences)\n",
    "The DNA sequences that we used are from \"concatenated.fasta\". This file is generated as explained \n",
    "[here](https://github.com/solislemuslab/dna-nn/blob/master/notebook.md) with OG_X.aligned.fasta files, one per OG.\n",
    "\n",
    "The result/response file that we use used is Perron_predictAMR_2021_updated.xlsx, received in Dec. 6th, 2021 by Email. We used all 8 parameters for regression. We processed the new response with this [jupyter notebook](https://github.com/solislemuslab/dna-nn/blob/master/nn-methods/yuren/new-data-process.ipynb) to csv files before training models on CHTC server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data encoding (how are sequences encoded)\n",
    "\n",
    "The data are first encoded into DNA words (3 nucleotides as one unit), which is done before in DNA-NN-Theory as inspired by this [paper](http://dx.doi.org/10.4236/jbise.2016.95021). For examnple, the input sequence 'ATGCA', will be eoncoded into words 'ATG', 'TGC', 'GCA' and each word will be represented by an integer. This will encode the DNA sequences with length N into the word sequence with length N-2. Then, we use one-hot encoding on the resulting word sequence the same as [DNA-NN-Theory paper](https://arxiv.org/abs/2012.05995).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splitting\n",
    "\n",
    "We randomly splitted the data to use 70% of dataset for training and remaining 15% for validating and 15% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation and simulation\n",
    "\n",
    "We augmented the data by simulating sequences with iqtree2 from concatenated.fasta with commend \"iqtree2 --alisim alignment_mimic -s path-to-concatenated.fasta.\" This will doulbe our sample size. Then, we augmented the data with reverse complement, which is inspired by this [paper](https://doi.org/10.1016/j.ab.2021.114120), to double the sample size again.\n",
    "\n",
    "We augment the data with simulated sequences, we generate response data (output of the model) by randomly selecting from the seperated distribution. More specifically, we seperate the original responses into 2 class (less susceptible or more susceptible) with data in tab \"binarized succeptibility key\", compute mean and sd for both two classes seperatedly, and select randomly from the normal distribution seperatedly (e.g. if the original response is from less susceptible group, we select ramdonly from the distribution computed by responses that are less susceptable for the simulated sequence). \n",
    "\n",
    "Besides, we tried not using any data augmentation and bootstraping as inspired by this [paper](https://www.pnas.org/content/93/23/13429). However, these do not work for our model (the training loss did not decrease at all, which means that the model was not albe to be trained)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN architecture\n",
    "\n",
    "We adapted from the model cnn_nguyen_conv1d_2_conv2d from the paper for DNA-NN-Theory and [github page](https://github.com/solislemuslab/dna-nn-theory/blob/master/cnn/dna_nn/model.py#L23).\n",
    "\n",
    "To modified to model for regression, we change the final layer to a linear dense layer, i.e., from Dense(1, activation='sigmoid') to Dense(1, activation=\"linear\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when was learning stopped? Early stopped or fixed number of epochs\n",
    "\n",
    "During the training process, we set the number of training epoches to 300. We also set earlystopping with patience as 50 epoches on validation loss so that the process will end with 50 epoches without reduction on validation loss (reducing validation loss means that the model is still learning from the training data and valudation results are closer to the expected output. At the end of each epoch, we validate the model and save the model if the validation loss is lower than those of all the epoches before so that we only save the best model. The model usually stop after trainig for around 100 epoches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer used, tuning parameters\n",
    "\n",
    "We used the adam optimizer and default learning rate (0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were trained on CHTC servers with the scripts [here](https://github.com/solislemuslab/dna-nn/tree/master/nn-methods/yuren/CHTC-scripts/cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
