{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7605,"status":"ok","timestamp":1634913982284,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"yS89mi0Exh8a"},"outputs":[],"source":["import h5py\n","import pandas as pd\n","import numpy as np\n","from itertools import product, islice\n","import gc\n","import random\n","\n","from sklearn.metrics import precision_recall_curve, roc_curve\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import tensorflow as tf\n","from tensorflow.data import Dataset\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Conv1D, Conv2D, Embedding, Dense, LSTM\n","from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalMaxPooling2D, MaxPooling1D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Input, Reshape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1634913982284,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"sI29AlLHt9my","outputId":"da8adebd-c6ee-4b72-ad3b-9fdf7ff98c0c"},"outputs":[],"source":["print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"95NflegNxh8h"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1634913982285,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"ROmkT7RNxnpa","outputId":"c206fe92-1fbc-46d2-cf46-6fc6b43653e2"},"outputs":[],"source":["# If run in Google Colab\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1368,"status":"ok","timestamp":1634913983648,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"Svs_TNRuxh8k"},"outputs":[],"source":["# Features, x\n","feature_raw = h5py.File(\"../../../data2/features.jld\", \"r\")\n","# feature_raw = h5py.File(\"My Drive/DNA-NN/features.jld\", \"r\")\n","features = np.array(feature_raw[\"features\"])\n","\n","# Remove the cols that all samples have the same site.\n","features = features[:, ~np.all(features[1:] == features[:-1], axis=0)]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1634913983650,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"lkDXD0vPxh8l"},"outputs":[],"source":["# Labels, y, 0 for false, 1 for True\n","response_raw = pd.read_csv(\"../../../data2/responses.csv\").dropna()\n","# response_raw = pd.read_csv(\"My Drive/DNA-NN/responses.csv\").dropna()\n","carb_tf = np.array(response_raw[\"carb\"])\n","toby_tf = np.array(response_raw[\"toby\"])\n","print(len(carb_tf))"]},{"cell_type":"markdown","metadata":{"id":"MAm_o75l5eTp"},"source":["# Split train, val, test and One hot coding"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1634913983651,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"0b87moNO5f8O"},"outputs":[],"source":["# One hot encoding, need to append to 10 for conv2d and 20 to conv\n","x = np.zeros((len(features),len(features[0]), 5))\n","for i in range(len(features)):\n","  x[i, range(len(features[i])), features[i]] = 1\n","\n","# # If embeded, not used\n","# x = np.array(features)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = np.array([1 if x else 0 for x in carb_tf])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index_1 = np.argwhere(y == 1).flatten().tolist()\n","index_0 = np.argwhere(y == 0).flatten().tolist()\n","print(len(index_1), len(index_0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split x and y\n","random.shuffle(index_1)\n","random.shuffle(index_0)\n","train_ratio = 0.7\n","test_val_ratio = 0.15\n","\n","# Ratio for true:false class after augmentation\n","# Used in training for a balanced dataset\n","# and use the whole dataset for val and test\n","# With augmentation to 4 * sample sizes\n","\n","ratio_1_0 = len(index_1) / len(index_0) * 4\n","index_0_shorten = index_0[:int(len(index_0)*ratio_1_0)]\n","\n","train_index = index_1[:int(train_ratio * len(index_1))] + \\\n","    index_0_shorten[:int(train_ratio * len(index_0_shorten))]\n","test_index = index_1[int(train_ratio * len(index_1)):int((train_ratio+test_val_ratio) * len(index_1))] + \\\n","    index_0[int(train_ratio * len(index_0)):int((train_ratio+test_val_ratio) * len(index_0))]\n","val_index = index_1[int((train_ratio+test_val_ratio) * len(index_1)):] + \\\n","    index_0[int((train_ratio+test_val_ratio) * len(index_0)):]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"train:\", len(train_index), train_index)\n","print(\"val:\", len(val_index), val_index)\n","print(\"test:\", len(test_index),test_index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train, y_train = x[train_index].tolist(), y[train_index].tolist()\n","x_test, y_test = x[test_index], y[test_index]\n","x_val, y_val = x[val_index], y[val_index]\n","print(len(x_train), len(x_test), len(x_val))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Augment the train index for true class (class 1)\n","def aug_reverse_complement(x_train, y_train, index_1, train_ratio, features):\n","  train_index_1 = index_1[:int(train_ratio * len(index_1))]\n","  features_train_1 = features[train_index_1]\n","\n","  # 1 - A, 2 - C, 3 - G, 4 - T, 0 - missing\n","  dic_complement = {0: 0, 1: 4, 2: 3, 3: 2, 4: 1}\n","\n","  for feature in features_train_1:\n","    feature_new = [dic_complement[x] for x in np.flip(feature)]\n","    # One hot encoding\n","    feature_encode = np.zeros((len(feature_new), 5))\n","    feature_encode[range(len(feature_new)), feature_new] = 1\n","    # Append to trainig set\n","    y_train = np.append(y_train, [1], axis=0)  # class for true\n","    x_train = np.append(x_train, [feature_encode], axis=0)\n","  \n","  return x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def aug_bootstrap(x_train, y_train):\n","  # Randomly shuffle column for multiple times (100 for now)\n","  # and append to the original matrix\n","  index_1 = np.argwhere(y_train == 1).flatten().tolist()\n","\n","  new_x_train_1 = np.transpose(np.copy(np.array(x_train)[index_1]))\n","  # for i in range(100):\n","  np.random.shuffle(new_x_train_1)\n","  \n","  x_train = np.append(x_train, np.transpose(np.array(new_x_train_1)), axis=0)\n","  y_train = np.append(y_train, y_train[index_1], axis=0)\n","\n","  return x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train_new, y_train_new = aug_reverse_complement(x_train, y_train, index_1, train_ratio, features)\n","x_train_new, y_train_new = aug_bootstrap(x_train_new, y_train_new)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(x_train_new), len(x_train_new[30]), len(y_train_new), y_train_new"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1634913983653,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"f1zYgueQf-xg"},"outputs":[],"source":["# Not used\n","# x_train, x_testVal, y_train, y_testVal = train_test_split(x, y, test_size=0.3)\n","# x_test, x_val, y_test, y_val = train_test_split(x_testVal, y_testVal, test_size=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1634913983839,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"Fmle5I8i027A","outputId":"ba363cb6-1a78-4c60-dea1-b1725d51d110"},"outputs":[],"source":["print(\"train, test, val len:\", len(x_train), len(x_test), len(x_val))"]},{"cell_type":"markdown","metadata":{"id":"6ZHKdBZvxh8o"},"source":["# Fitting models\n","From https://github.com/solislemuslab/dna-nn-theory/blob/master/cnn/dna_nn/model.py\n","\n","Crash on one-hot encoding combinging encoding sites into words of 3 sites. "]},{"cell_type":"markdown","metadata":{},"source":["## Deepram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def deepram_conv1d_recurrent_onehot(x_shape, classes=2):\n","    model = keras.Sequential([\n","        Input(shape=x_shape),\n","        Dropout(0.5),\n","        Conv1D(64, 3, activation='relu'),\n","        MaxPooling1D(),\n","        Conv1D(128, 3, activation='relu'),\n","        MaxPooling1D(),\n","        LSTM(64, return_sequences=True),\n","        LSTM(128),\n","        Dense(128),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid') if classes < 3 else Dense(\n","            classes, activation='softmax')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1634913989730,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"LWknrtXOxh8q"},"outputs":[],"source":["def deepram_recurrent_onehot(x_shape, classes=2):\n","    model = keras.Sequential([\n","        Input(shape=x_shape),\n","        Dropout(0.5),\n","        LSTM(16, return_sequences=True),\n","        LSTM(32),\n","        Dense(32),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid') if classes < 3 else Dense(classes, activation='softmax')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1634913989731,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"OI6SImqBsKQQ"},"outputs":[],"source":["def deepram_recurrent_embed(x_shape, classes=2):\n","    model = keras.Sequential([\n","        Input(shape=(x_shape)),\n","        Embedding(x_shape, 3),\n","        Dropout(0.5),\n","        LSTM(8, return_sequences=True),\n","        LSTM(16),\n","        Dense(16),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid') if classes < 3 else Dense(classes, activation='softmax')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1634914034767,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"y7s3menexh8r","outputId":"cb69e4d1-72a1-43e9-b6be-c1b7858c5970"},"outputs":[],"source":["model = None\n","keras.backend.clear_session()\n","# x_shape = (len(x_train[0]), len(x_train[0][0]))\n","# model = deepram_recurrent_onehot(x_shape)\n","# model = deepram_recurrent_embed(len(x_train[0]))\n","model = deepram_conv1d_recurrent_onehot(x_shape)\n","# model = cnn_nguyen_conv1d_2_conv2d(x_shape)\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pglBdOL1htE","outputId":"0a9afcea-77fc-40b9-9477-657ec359ee86"},"outputs":[],"source":["LOG_DIR = \"My Drive/DNA-NN/\"\n","\n","csv_path = LOG_DIR + 'DeepRam-dynamics.csv'\n","model_path = LOG_DIR + 'DeepRam.h5'\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(model_path, save_best_only=True),\n","    keras.callbacks.CSVLogger(csv_path),\n","    keras.callbacks.LambdaCallback(\n","        on_epoch_end=lambda epoch, logs: gc.collect(),\n","        # on_train_end=lambda logs: model.save(model_path)\n","    )\n","]\n","\n","# Currently takes 20 - 30 min per epoch, will convert to Python script to run on CHTC.\n","history = model.fit(np.array(x_train), np.array(y_train), epochs=50, validation_data=( np.array(x_val), np.array(y_val) ),\n","                    callbacks=callbacks, verbose=1, batch_size=4)"]},{"cell_type":"markdown","metadata":{},"source":["## CNN Nguyen\n","Currently overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":181,"status":"ok","timestamp":1634913984013,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"XgGa69NLMjln"},"outputs":[],"source":["# Try using dataset generator to avoid crashing or OOM\n","def encoded_shape(x_len, word_size=3, region_size=0, onehot=True, expand=True, alphabet='01234'):\n","    '''calculate the shape of encoding base on the sequence length'''\n","    dim_1 = x_len - word_size + 1\n","    dim_2 = ((len(alphabet) ** word_size) if onehot else 1) * (region_size + 1)\n","    if not region_size and not onehot:\n","        return (dim_1, 1) if expand else (dim_1,)\n","    return (dim_1, dim_2, 1) if expand else (dim_1, dim_2)\n","\n","def gen_from_arrays(features, labels, word_size=3, region_size=0, onehot=True, expand=True, alphabet='01234'):\n","  words = [''.join(p) for p in product(alphabet, repeat=word_size)]\n","  word_to_idx = {word: i for i, word in enumerate(words)}\n","  word_to_idx_func = np.vectorize(lambda word: word_to_idx[word], otypes=[np.int8])\n","  def gen():\n","   for x, y in zip(features, labels):\n","    #  one hot encoding to size 10\n","      x = [\"\".join(map(str,x[i:i+word_size])) for i in range(len(x) - word_size + 1)]\n","      idx = word_to_idx_func(list(x))\n","      processed_x = np.zeros((len(idx), len(word_to_idx)))\n","      processed_x[range(len(idx)), idx] = 1\n","      processed_x = np.expand_dims(processed_x, axis=-1)\n","      yield processed_x, y\n","  return gen\n","\n","train_gen = gen_from_arrays(x_train, y_train)\n","val_gen = gen_from_arrays(x_val, y_val)\n","test_gen = gen_from_arrays(x_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5714,"status":"ok","timestamp":1634913989726,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"D-xZ7tMiNxkL"},"outputs":[],"source":["# datasets\n","batch_size = 4\n","prefetch = tf.data.experimental.AUTOTUNE\n","\n","x_shape = encoded_shape(len(features[0]))\n","output_shapes = (x_shape, ())\n","output_types = (tf.float32, tf.float32)\n","\n","train_ds = Dataset.from_generator(train_gen, output_types, output_shapes)\n","train_ds = train_ds.shuffle(500).batch(batch_size).prefetch(prefetch)\n","\n","test_ds = Dataset.from_generator(test_gen, output_types, output_shapes)\n","test_ds = test_ds.batch(batch_size).prefetch(prefetch)\n","\n","val_ds = Dataset.from_generator(val_gen, output_types, output_shapes)\n","val_ds = train_ds.shuffle(500).batch(batch_size).prefetch(prefetch)\n","\n","x_val_encode, y_val_encode = [], []\n","for x, y in val_gen():\n","    x_val_encode.append(x)\n","    y_val_encode.append(y)\n","x_val_encode = np.array(x_val_encode)\n","y_val_encode = np.array(y_val_encode)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1634913989731,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"r7hBgUFM43ku"},"outputs":[],"source":["def cnn_nguyen_conv1d_2_conv2d(x_shape, classes=2):\n","    strides = (x_shape[0] - x_shape[1] + 1, 1) if x_shape[0] > x_shape[1] else (1, x_shape[1] - x_shape[0] + 1)\n","    model = keras.Sequential([\n","        Conv2D(16, strides, activation='relu', input_shape=x_shape),\n","        MaxPooling2D(),\n","        Conv2D(16, 3, activation='relu'),\n","        MaxPooling2D(),\n","        Conv2D(32, 3, activation='relu'),\n","        MaxPooling2D(),\n","        Flatten(),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid') if classes < 3 else Dense(classes, activation='softmax')\n","    ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":545,"status":"ok","timestamp":1634913990255,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"-eOZLhdS0EMN"},"outputs":[],"source":["# Config tf for InternalError, \n","# Failed to call ThenRnnForward with model config:InternalError\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","config.gpu_options.per_process_gpu_memory_fraction = 0.8\n","tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1634914034767,"user":{"displayName":"yuren sun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07174142991493523809"},"user_tz":300},"id":"y7s3menexh8r","outputId":"cb69e4d1-72a1-43e9-b6be-c1b7858c5970"},"outputs":[],"source":["model = None\n","keras.backend.clear_session()\n","# x_shape = (len(x_train[0]), len(x_train[0][0]))\n","model = cnn_nguyen_conv1d_2_conv2d(x_shape)\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pglBdOL1htE","outputId":"0a9afcea-77fc-40b9-9477-657ec359ee86"},"outputs":[],"source":["LOG_DIR = \"My Drive/DNA-NN/\"\n","\n","csv_path = LOG_DIR + 'DeepRam-dynamics.csv'\n","model_path = LOG_DIR + 'DeepRam.h5'\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(model_path, save_best_only=True),\n","    keras.callbacks.CSVLogger(csv_path),\n","    keras.callbacks.LambdaCallback(\n","        on_epoch_end=lambda epoch, logs: gc.collect(),\n","        # on_train_end=lambda logs: model.save(model_path)\n","    )\n","]\n","\n","# Use tf dataset to generate data separatedly for one hot encoding\n","# after encode sites into words of 3 sites, still crash\n","history = model.fit(train_ds, epochs=50, validation_data=(x_val_encode, y_val_encode),\n","                    callbacks=callbacks, verbose=1, batch_size=4)\n","\n","# Fit raw data without encoding sites into words, overfitting.\n","history = model.fit(np.array(x_train), np.array(y_train), epochs=50, validation_data=( np.array(x_val), np.array(y_val) ),\n","                    callbacks=callbacks, verbose=1, batch_size=4)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CNN-RNN.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"4ac6ed32b699bac43e826bfc5060e6da7d580dd930d3c77829d5a728b6b6b07b"},"kernelspec":{"display_name":"Python 3.7.3 64-bit (conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
