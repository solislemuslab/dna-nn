{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explanation of data (file it came from, which response is being predicted, which sequences)\n",
    "The DNA sequences that we used are from \"concatenated.fasta\". This file is generated as explained \n",
    "[here](https://github.com/solislemuslab/dna-nn/blob/master/notebook.md) with OG_X.aligned.fasta files, one per OG.\n",
    "\n",
    "The result file that we use used is Perron_predictAMR_2021_updated.xlsx, received in Dec. 6th, 2021 by Email. We used all 8 parameters for classification and binarized the data into 2 class (less susceptible or more susceptible) with data in tab \"binarized succeptibility key\". Among them, only classifications on the measurements delta.carb.max.od1 and delta.toby.max.rate works with testing accuracy around 70-75%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data encoding (how are sequences encoded)\n",
    "\n",
    "The data are first encoded into DNA words (3 nucleotides as one unit), which is done before in DNA-NN-Theory as inspired by this [paper](http://dx.doi.org/10.4236/jbise.2016.95021). For examnple, the input sequence 'ATGCA', will be eoncoded into words 'ATG', 'TGC', 'GCA' and each word will be represented by an integer. This will encode the DNA sequences with length N into the word sequence with length N-2. Then, we use one-hot encoding on the resulting word sequence the same as [DNA-NN-Theory paper](https://arxiv.org/abs/2012.05995).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splitting\n",
    "\n",
    "We randomly splitted the data to use 70% of dataset for testing and remaining 15% for validating and 15% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation and simulation\n",
    "\n",
    "We augmented the data by simulating sequences with iqtree2 from concatenated.fasta with commend \"iqtree2 --alisim alignment_mimic -s path-to-concatenated.fasta.\" This will doulbe our sample size. Then, we augmented the data with reverse complement, which is inspired by this [paper](https://doi.org/10.1016/j.ab.2021.114120), to doulbe the sample size again.\n",
    "\n",
    "Besides, we tried bootstraping as inspired by this [paper](https://www.pnas.org/content/93/23/13429). However, this does not work for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN architecture\n",
    "\n",
    "We used the model the same model cnn_nguyen_conv1d_2_conv2d as the paper for DNA-NN-Theory and [github page](https://github.com/solislemuslab/dna-nn-theory/blob/master/cnn/dna_nn/model.py#L23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when was learning stopped? Early stopped or fixed number of epochs\n",
    "\n",
    "During the training process, we set the number of training epoches to 300. We also set earlystopping with patience as 50 epoches on validation loss so that the process will end with 50 epoches without reduction on validation loss (reducing validation loss means that the model is still learning from the training data and valudation results are closer to the expected output. At the end of each epoch, we validate the model and save the model if the validation loss is lower than those of all the epoches before so that we only save the best model. The model usually stop after trainig for around 100 epoches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer used, tuning parameters like learning rate, patience, others\n",
    "\n",
    "We used the adam optimizer, default learning rate (0.001). The patience for early stopping is 50 as described before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/testing accuracy (save to csv file so that we can redo plots or calculate other measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
